{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "bjlnhjrefdsdy6zg7t4p",
   "authorId": "7463897734774",
   "authorName": "AKEENANTFCDOP1023",
   "authorEmail": "akeenan@texasfcs.com",
   "sessionId": "6b45a02b-df5b-44bd-b789-99b6e800b19f",
   "lastEditTime": 1770144920130
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "227f717b-023d-4808-aa6c-a2a9572db5b9",
   "metadata": {
    "name": "NOTEBOOK_OVERVIEW",
    "collapsed": false
   },
   "source": "# PRF Premium Rates Pipeline — User Guide\n\n## What This Pipeline Does\n\nThis notebook pulls **PRF premium rates** from the USDA RMA public API and loads them into Snowflake. Premium rates represent how risky a given grid/interval is — they're one of the key inputs to the Champion vs. Challenger Streamlit app.\n\n**Destination Table:** `CAPITAL_MARKETS_SANDBOX.PUBLIC.PRF_PREMIUM_RATES`\n\n---\n\n## Quick Start (Copy & Paste)\n\nIf you just want to load all Texas grids for 2025 with default settings:\n\n```sql\nCALL RUN_PRF_PREMIUM_PIPELINE(\n    ARRAY_CONSTRUCT('Texas'),\n    NULL,\n    ARRAY_CONSTRUCT(2025),\n    ARRAY_CONSTRUCT(70, 75, 80, 85, 90),\n    'Grazing', '007', '997', '997',\n    '[50,0,50,0,0,0,0,0,0,0,0]',\n    5, 60, 'OVERWRITE'\n);\n```\n\n---\n\n## Notebook Cells (Run in Order)\n\n| Cell | Name | What It Does |\n|------|------|-------------|\n| 1 | **SETUP** | Sets Snowflake context, creates network rules, creates/replaces the target table |\n| 2 | **PROCEDURE** | Creates the `RUN_PRF_PREMIUM_PIPELINE` stored procedure |\n| 3 | **EXECUTE** | Where you set your parameters and call the procedure |\n| 4 | **AUDIT** | Queries to verify your data loaded correctly |\n\n> **First time?** Run all 4 cells in order.\n> **Refreshing data?** Skip to Cell 3 (EXECUTE) — the procedure already exists.\n\n---\n\n## Parameters (Cell 3)\n\nThese are the 12 parameters you can configure in the EXECUTE cell:\n\n### Data Selection\n\n| # | Parameter | What It Controls | Example |\n|---|-----------|-----------------|---------|\n| 1 | `STATE_NAMES` | Which states to load (auto-resolves grids) | `ARRAY_CONSTRUCT('Texas')` |\n| 2 | `GRID_IDS` | Specific grids (use instead of states) | `ARRAY_CONSTRUCT(9128, 9129)` |\n| 3 | `YEARS` | Policy years to fetch | `ARRAY_CONSTRUCT(2025)` |\n| 4 | `COVERAGE_LEVELS` | Coverage percentages | `ARRAY_CONSTRUCT(70, 75, 80, 85, 90)` |\n\n> **Rule:** Use `STATE_NAMES` OR `GRID_IDS` — set the other one to `NULL`.\n\n### Policy Settings\n\n| # | Parameter | Default | Notes |\n|---|-----------|---------|-------|\n| 5 | `INTENDED_USE` | `'Grazing'` | Friendly label (`'Grazing'` or `'Haying'`) |\n| 6 | `INTENDED_USE_CODE` | `'007'` | `'007'` = Grazing, `'006'` = Haying |\n| 7 | `IRRIGATION_PRACTICE` | `'997'` | `'997'` = Non-irrigated (standard) |\n| 8 | `ORGANIC_PRACTICE` | `'997'` | `'997'` = Not specified (standard) |\n| 9 | `INTERVAL_PERCENT` | `'[50,0,50,0,0,0,0,0,0,0,0]'` | Allocation across 11 intervals |\n\n> For most PRF use cases, parameters 7–9 stay at their defaults.\n\n### Performance & Mode\n\n| # | Parameter | Default | Notes |\n|---|-----------|---------|-------|\n| 10 | `MAX_CONCURRENT` | `5` | Parallel API threads (5–10 recommended) |\n| 11 | `REQUEST_TIMEOUT` | `60` | Seconds per API call before timeout |\n| 12 | `MODE` | `'OVERWRITE'` | How data is written (see below) |\n\n---\n\n## Load Modes Explained\n\n| Mode | What It Does | When to Use |\n|------|-------------|-------------|\n| `OVERWRITE` | Drops all existing data, loads fresh | First load or full refresh |\n| `APPEND` | Adds rows without checking for duplicates | Use carefully — can create duplicates |\n| `MERGE` | Inserts new rows, updates existing ones | Adding new states/grids to existing data |\n\n**Recommended workflow:**\n1. **First load →** `OVERWRITE`\n2. **Adding more states later →** `MERGE`\n3. **Annual refresh (new year's rates) →** `MERGE`\n\n---\n\n## Common Scenarios\n\n### Load all grids for one state\n```sql\nCALL RUN_PRF_PREMIUM_PIPELINE(\n    ARRAY_CONSTRUCT('Texas'), NULL,\n    ARRAY_CONSTRUCT(2025),\n    ARRAY_CONSTRUCT(70, 75, 80, 85, 90),\n    'Grazing', '007', '997', '997',\n    '[50,0,50,0,0,0,0,0,0,0,0]',\n    5, 60, 'OVERWRITE'\n);\n```\n\n### Load multiple states at once\n```sql\nCALL RUN_PRF_PREMIUM_PIPELINE(\n    ARRAY_CONSTRUCT('Texas', 'Oklahoma', 'New Mexico'), NULL,\n    ARRAY_CONSTRUCT(2025),\n    ARRAY_CONSTRUCT(70, 75, 80, 85, 90),\n    'Grazing', '007', '997', '997',\n    '[50,0,50,0,0,0,0,0,0,0,0]',\n    10, 60, 'OVERWRITE'\n);\n```\n> Bump `MAX_CONCURRENT` to 10 for multi-state loads.\n\n### Load only King Ranch grids\n```sql\nCALL RUN_PRF_PREMIUM_PIPELINE(\n    NULL,\n    ARRAY_CONSTRUCT(9128, 9129, 9130, 9131, 8828, 8829, 8830, 8831,\n                    8528, 8529, 8530, 8531, 8228, 8229, 8230, 8231,\n                    7928, 7929, 7930, 7931),\n    ARRAY_CONSTRUCT(2025),\n    ARRAY_CONSTRUCT(70, 75, 80, 85, 90),\n    'Grazing', '007', '997', '997',\n    '[50,0,50,0,0,0,0,0,0,0,0]',\n    5, 60, 'MERGE'\n);\n```\n\n### Add a new state without wiping existing data\n```sql\nCALL RUN_PRF_PREMIUM_PIPELINE(\n    ARRAY_CONSTRUCT('Colorado'), NULL,\n    ARRAY_CONSTRUCT(2025),\n    ARRAY_CONSTRUCT(70, 75, 80, 85, 90),\n    'Grazing', '007', '997', '997',\n    '[50,0,50,0,0,0,0,0,0,0,0]',\n    5, 60, 'MERGE'\n);\n```\n\n---\n\n## Table Creation Mode (Cell 1)\n\nCell 1 has a toggle at the top:\n\n```sql\nSET TABLE_MODE = 1;\n```\n\n| Mode | Behavior |\n|------|----------|\n| `1` | **Clean slate** — drops and recreates the table (use for first setup or schema changes) |\n| `2` | **Preserve data** — keeps existing rows, adds any new columns |\n\n> After your first run, switch to `TABLE_MODE = 2` so you don't accidentally wipe data.\n\n---\n\n## How to Verify Your Data (Cell 4)\n\nCell 4 runs three audit queries:\n\n1. **Summary by state** — shows grid counts, year counts, and total rows per state\n2. **Completeness check** — flags any grid/year combos that don't have the expected 55 rows (11 intervals × 5 coverage levels). If anything shows up here, those grids had API errors — re-run them with `MERGE` mode.\n3. **Sample records** — eyeball check on actual data\n\n---\n\n## Error Handling\n\nThe pipeline won't crash if some grids fail. Here's what happens:\n\n- **Bad state name →** Returns an error immediately, nothing loads\n- **Grid missing from MAP_YTD →** Warns you, skips that grid, processes the rest\n- **API call fails for one grid →** Logs the error, keeps going with other grids\n- **API returns empty data →** Logged as an error, no rows written for that combo\n\nAt the end you get a summary like:\n```\nCOMPLETE | Mode: OVERWRITE | 487 grids | 26,785 rows | 3 errors | 142.5s\n```\n\nIf you have errors, just re-run those specific grids with `MERGE` mode.\n\n---\n\n## Performance Tips\n\n| Scenario | Grids | Est. API Calls | Suggested MAX_CONCURRENT | Est. Time |\n|----------|-------|---------------|--------------------------|-----------|\n| King Ranch only | 20 | 100 | 5 | ~30s |\n| Single state (small) | ~50 | 250 | 5 | ~1 min |\n| Texas (all grids) | ~500 | 2,500 | 10 | ~5–10 min |\n| Multi-state | ~1,000+ | 5,000+ | 10 | ~15–20 min |\n\n> **Formula:** API calls = grids × years × coverage levels (e.g., 500 × 1 × 5 = 2,500)\n\n---\n\n## Where This Fits\n\n```\n┌─────────────────────────────────────────────┐\n│           Preparation Pipelines             │\n├─────────────────────────────────────────────┤\n│  1. FTP Ingestion (ADM source files)        │\n│  2. Rain Index    → RAIN_INDEX_PLATINUM_ENH │\n│  3. Subsidies     → SUBSIDYPERCENT_PLATINUM │\n│  4. Premiums      → PRF_PREMIUM_RATES    ◄──── THIS PIPELINE\n│  5. County Base   → PRF_COUNTY_BASE_VALUES  │\n└──────────────────┬──────────────────────────┘\n                   │\n                   ▼\n        Streamlit PRF Application\n     (Champion vs Challenger Engine)\n```\n\nPremium rates feed directly into the app's ROI calculations:\n**Protection × Premium Rate = Total Premium → minus Subsidy = Producer Premium**"
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "PREMIUMS_STEP_1"
   },
   "source": "-- ====================================================================\n-- SETUP: Context, Network Rules, Target Table, Log Table\n-- ====================================================================\nUSE ROLE ACCOUNTADMIN;\nUSE WAREHOUSE COMPUTE_WH;\nUSE DATABASE CAPITAL_MARKETS_SANDBOX;\nUSE SCHEMA PUBLIC;\n\n-- ====================================================================\n-- PARAMETER: TABLE CREATION MODE\n--   1 = CREATE OR REPLACE (clean slate, drops existing data)\n--   2 = ALTER TABLE (preserve existing data, add new columns)\n-- ====================================================================\nSET TABLE_MODE = 1;\n\n-- Network Rule (shared with Rain Index pipeline)\nCREATE OR REPLACE NETWORK RULE usda_api_network_rule\n    MODE = EGRESS\n    TYPE = HOST_PORT\n    VALUE_LIST = ('public-rma.fpac.usda.gov');\n\nCREATE OR REPLACE EXTERNAL ACCESS INTEGRATION usda_api_integration\n    ALLOWED_NETWORK_RULES = (usda_api_network_rule)\n    ENABLED = TRUE;\n\n-- Pipeline Log Table (always replace — it's just a monitoring tool)\nCREATE OR REPLACE TABLE CAPITAL_MARKETS_SANDBOX.PUBLIC.PRF_PIPELINE_LOG (\n    LOG_TIME        TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n    STEP            VARCHAR,\n    MESSAGE         VARCHAR\n);\n\n-- Target Table\nEXECUTE IMMEDIATE $$\nBEGIN\n    IF ($TABLE_MODE = 1) THEN\n        CREATE OR REPLACE TABLE CAPITAL_MARKETS_SANDBOX.PUBLIC.PRF_PREMIUM_RATES (\n            GRID_ID                 NUMBER,\n            STATE_CODE              VARCHAR,\n            COUNTY_CODE             VARCHAR,\n            INTENDED_USE            VARCHAR,\n            COVERAGE_LEVEL          VARCHAR,\n            YEAR                    NUMBER,\n            INDEX_INTERVAL_CODE     VARCHAR,\n            INDEX_INTERVAL_NAME     VARCHAR,\n            PREMIUMRATE             FLOAT,\n            INSERT_TIMESTAMP        TIMESTAMP_NTZ\n        );\n        RETURN 'Mode 1: Table replaced (clean slate).';\n    ELSE\n        CREATE TABLE IF NOT EXISTS CAPITAL_MARKETS_SANDBOX.PUBLIC.PRF_PREMIUM_RATES (\n            GRID_ID                 NUMBER,\n            INTENDED_USE            VARCHAR,\n            COVERAGE_LEVEL          VARCHAR,\n            YEAR                    NUMBER,\n            INDEX_INTERVAL_CODE     VARCHAR,\n            INDEX_INTERVAL_NAME     VARCHAR,\n            PREMIUMRATE             FLOAT,\n            INSERT_TIMESTAMP        TIMESTAMP_NTZ\n        );\n        ALTER TABLE CAPITAL_MARKETS_SANDBOX.PUBLIC.PRF_PREMIUM_RATES\n            ADD COLUMN IF NOT EXISTS STATE_CODE VARCHAR;\n        ALTER TABLE CAPITAL_MARKETS_SANDBOX.PUBLIC.PRF_PREMIUM_RATES\n            ADD COLUMN IF NOT EXISTS COUNTY_CODE VARCHAR;\n        RETURN 'Mode 2: Existing data preserved, new columns added.';\n    END IF;\nEND;\n$$;",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "b9f6bf8c-41a7-4f8e-b5b2-380afd079415",
   "metadata": {
    "language": "sql",
    "name": "PREMIUMS_STEP_2"
   },
   "outputs": [],
   "source": "-- ====================================================================\n-- MASTER PROCEDURE: RUN_PRF_PREMIUM_PIPELINE (V2)\n-- ====================================================================\n-- Upgrades from V1:\n--   - requests.Session() for connection pooling (matches Rain Index)\n--   - GRID_BATCH parameter for parallel cell execution\n--   - Retry with backoff on failed API calls\n--   - Longer default timeout (180s, matches Rain Index)\n--   - Real-time progress logging to PRF_PIPELINE_LOG\n-- ====================================================================\n\nCREATE OR REPLACE PROCEDURE CAPITAL_MARKETS_SANDBOX.PUBLIC.RUN_PRF_PREMIUM_PIPELINE(\n    STATE_NAMES             ARRAY    DEFAULT NULL,\n    GRID_IDS                ARRAY    DEFAULT NULL,\n    YEARS                   ARRAY    DEFAULT ARRAY_CONSTRUCT(2025),\n    COVERAGE_LEVELS         ARRAY    DEFAULT ARRAY_CONSTRUCT(70, 75, 80, 85, 90),\n    INTENDED_USE            VARCHAR  DEFAULT 'Grazing',\n    INTENDED_USE_CODE       VARCHAR  DEFAULT '007',\n    IRRIGATION_PRACTICE     VARCHAR  DEFAULT '997',\n    ORGANIC_PRACTICE        VARCHAR  DEFAULT '997',\n    INTERVAL_PERCENT        VARCHAR  DEFAULT '[50,0,50,0,0,0,0,0,0,0,0]',\n    MAX_CONCURRENT          INT      DEFAULT 10,\n    REQUEST_TIMEOUT         INT      DEFAULT 180,\n    MODE                    VARCHAR  DEFAULT 'OVERWRITE',\n    GRID_BATCH              VARCHAR  DEFAULT NULL\n)\nRETURNS VARCHAR\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.10'\nPACKAGES = ('snowflake-snowpark-python', 'requests', 'pandas')\nHANDLER = 'main'\nEXTERNAL_ACCESS_INTEGRATIONS = (usda_api_integration)\nAS\n$$\nimport requests\nimport pandas as pd\nimport datetime\nimport snowflake.snowpark as snowpark\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport time\nimport json\nimport math\n\ndef main(session: snowpark.Session,\n         state_names: list = None,\n         grid_ids: list = None,\n         years: list = None,\n         coverage_levels: list = None,\n         intended_use: str = 'Grazing',\n         intended_use_code: str = '007',\n         irrigation_practice: str = '997',\n         organic_practice: str = '997',\n         interval_percent: str = '[50,0,50,0,0,0,0,0,0,0,0]',\n         max_concurrent: int = 10,\n         request_timeout: int = 180,\n         mode: str = 'OVERWRITE',\n         grid_batch: str = None) -> str:\n\n    pipeline_start = datetime.datetime.now()\n\n    LOG_TABLE = \"CAPITAL_MARKETS_SANDBOX.PUBLIC.PRF_PIPELINE_LOG\"\n\n    # ================================================================\n    # HELPER: LOG TO TABLE\n    # ================================================================\n    def log(step, message):\n        \"\"\"Write a progress row to PRF_PIPELINE_LOG.\"\"\"\n        safe_msg = message.replace(\"'\", \"''\")\n        safe_step = step.replace(\"'\", \"''\")\n        try:\n            session.sql(f\"\"\"\n                INSERT INTO {LOG_TABLE} (LOG_TIME, STEP, MESSAGE)\n                VALUES (CURRENT_TIMESTAMP(), '{safe_step}', '{safe_msg}')\n            \"\"\").collect()\n        except:\n            pass\n        print(f\"  [{step}] {message}\")\n\n    # ================================================================\n    # VALIDATE MODE\n    # ================================================================\n    mode = mode.upper()\n    if mode not in ['OVERWRITE', 'APPEND', 'MERGE']:\n        return f\"ERROR: Invalid MODE '{mode}'. Must be OVERWRITE, APPEND, or MERGE.\"\n\n    # ================================================================\n    # DEFAULTS\n    # ================================================================\n    if years is None:\n        years = [2025]\n    if coverage_levels is None:\n        coverage_levels = [70, 75, 80, 85, 90]\n\n    TARGET_TABLE = \"CAPITAL_MARKETS_SANDBOX.PUBLIC.PRF_PREMIUM_RATES\"\n    STAGING_TABLE = \"CAPITAL_MARKETS_SANDBOX.PUBLIC.PRF_PREMIUM_RATES_STAGING\"\n    BASE_URL = \"https://public-rma.fpac.usda.gov/apps/PrfWebApi/PrfExternalPricingRates/GetPricingRates\"\n\n    INTERVAL_MAP = {\n        \"625\": \"Jan-Feb\", \"626\": \"Feb-Mar\", \"627\": \"Mar-Apr\", \"628\": \"Apr-May\",\n        \"629\": \"May-Jun\", \"630\": \"Jun-Jul\", \"631\": \"Jul-Aug\", \"632\": \"Aug-Sep\",\n        \"633\": \"Sep-Oct\", \"634\": \"Oct-Nov\", \"635\": \"Nov-Dec\"\n    }\n\n    # Clear previous log entries only if NOT batching (avoid clearing other batch logs)\n    if grid_batch is None:\n        try:\n            session.sql(f\"TRUNCATE TABLE {LOG_TABLE}\").collect()\n        except:\n            pass\n\n    batch_label = f\" | Batch: {grid_batch}\" if grid_batch else \"\"\n    log(\"START\", f\"Pipeline started | Mode: {mode} | Use: {intended_use} | Years: {years} | Coverage: {coverage_levels}{batch_label}\")\n\n    # ================================================================\n    # STEP 1: RESOLVE GRIDS\n    # ================================================================\n    log(\"RESOLVE\", \"Resolving grids...\")\n\n    if grid_ids and len(grid_ids) > 0:\n        valid_grids = []\n        skipped_grids = []\n        for g in grid_ids:\n            try:\n                valid_grids.append(int(g))\n            except (ValueError, TypeError):\n                skipped_grids.append(str(g))\n\n        if skipped_grids:\n            log(\"RESOLVE\", f\"WARNING: Skipped {len(skipped_grids)} non-numeric grid IDs: {skipped_grids[:10]}\")\n\n        resolved_grids = valid_grids\n        log(\"RESOLVE\", f\"Using {len(resolved_grids)} explicitly provided grid IDs.\")\n\n    elif state_names and len(state_names) > 0:\n        states_upper = [s.strip().upper() for s in state_names]\n        states_sql = \",\".join([f\"'{s}'\" for s in states_upper])\n\n        grid_df = session.sql(f\"\"\"\n            SELECT DISTINCT\n                TRY_TO_NUMBER(GRID_ID) AS GRID_ID,\n                STATE_CODE,\n                STATE_ABBREVIATION\n            FROM CAPITAL_MARKETS_SANDBOX.PUBLIC.GRID_STATE_PRF\n            WHERE (UPPER(STATE_NAME) IN ({states_sql})\n               OR UPPER(STATE_ABBREVIATION) IN ({states_sql}))\n               AND TRY_TO_NUMBER(GRID_ID) IS NOT NULL\n            ORDER BY 1\n        \"\"\").to_pandas()\n\n        if grid_df.empty:\n            log(\"ERROR\", f\"No grids found for states: {state_names}\")\n            return f\"ERROR: No grids found for states: {state_names}\"\n\n        resolved_grids = [int(g) for g in grid_df['GRID_ID'].tolist()]\n        states_found = grid_df['STATE_ABBREVIATION'].unique().tolist()\n        log(\"RESOLVE\", f\"Resolved {len(resolved_grids)} numeric grids for states: {states_found}\")\n    else:\n        return \"ERROR: Must provide either STATE_NAMES or GRID_IDS.\"\n\n    if not resolved_grids:\n        return \"ERROR: No valid numeric grid IDs to process.\"\n\n    # ================================================================\n    # STEP 1B: APPLY GRID BATCH FILTER\n    # ================================================================\n    if grid_batch:\n        try:\n            parts = grid_batch.lower().replace(' ', '').split('of')\n            batch_num = int(parts[0])\n            batch_total = int(parts[1])\n\n            if batch_num < 1 or batch_num > batch_total:\n                return f\"ERROR: Invalid GRID_BATCH '{grid_batch}'. Batch number must be between 1 and {batch_total}.\"\n\n            resolved_grids.sort()\n            chunk_size = math.ceil(len(resolved_grids) / batch_total)\n            start_idx = (batch_num - 1) * chunk_size\n            end_idx = min(start_idx + chunk_size, len(resolved_grids))\n            all_grids_count = len(resolved_grids)\n            resolved_grids = resolved_grids[start_idx:end_idx]\n\n            log(\"BATCH\", f\"Batch {batch_num} of {batch_total}: grids {start_idx+1}-{end_idx} of {all_grids_count} ({len(resolved_grids)} grids)\")\n        except Exception as e:\n            return f\"ERROR: Invalid GRID_BATCH format '{grid_batch}'. Use '1 of 4', '2 of 4', etc. Error: {str(e)}\"\n\n    # ================================================================\n    # STEP 2: BUILD GRID → STATE/COUNTY LOOKUP\n    # ================================================================\n    log(\"LOOKUP\", \"Looking up state/county codes from MAP_YTD...\")\n\n    grid_list_sql = \",\".join([f\"'{g}'\" for g in resolved_grids])\n\n    lookup_df = session.sql(f\"\"\"\n        SELECT DISTINCT\n            TRY_TO_NUMBER(SUB_COUNTY_CODE) AS GRID_ID,\n            STATE_CODE,\n            COUNTY_CODE\n        FROM CAPITAL_MARKETS_SANDBOX.PUBLIC.MAP_YTD\n        WHERE SUB_COUNTY_CODE IN ({grid_list_sql})\n          AND TRY_TO_NUMBER(SUB_COUNTY_CODE) IS NOT NULL\n    \"\"\").to_pandas()\n\n    grid_lookup = {}\n    for _, row in lookup_df.iterrows():\n        grid_lookup[int(row['GRID_ID'])] = {\n            'state_code': str(row['STATE_CODE']),\n            'county_code': str(row['COUNTY_CODE'])\n        }\n\n    missing_grids = [g for g in resolved_grids if g not in grid_lookup]\n    if missing_grids:\n        log(\"LOOKUP\", f\"WARNING: {len(missing_grids)} grids not found in MAP_YTD: {missing_grids[:10]}...\")\n        resolved_grids = [g for g in resolved_grids if g in grid_lookup]\n\n    log(\"LOOKUP\", f\"Lookup ready for {len(resolved_grids)} grids.\")\n\n    # ================================================================\n    # STEP 3: BUILD API CALL LIST\n    # ================================================================\n    api_calls = []\n    for grid_id in resolved_grids:\n        info = grid_lookup[grid_id]\n        for year in years:\n            for cov in coverage_levels:\n                api_calls.append({\n                    'grid_id': grid_id,\n                    'state_code': info['state_code'],\n                    'county_code': info['county_code'],\n                    'year': int(year),\n                    'coverage_level': int(cov)\n                })\n\n    total_calls = len(api_calls)\n    log(\"BUILD\", f\"{len(resolved_grids)} grids x {len(years)} years x {len(coverage_levels)} coverage levels = {total_calls} API calls\")\n\n    # ================================================================\n    # STEP 4: FETCH DATA FROM API (PARALLEL + CONNECTION POOLING)\n    # ================================================================\n    log(\"FETCH\", f\"Starting API fetch ({max_concurrent} concurrent, connection pooling enabled)...\")\n\n    all_rows = []\n    errors = []\n    retried = 0\n    run_timestamp = datetime.datetime.now()\n\n    # Shared HTTP session for connection pooling (matches Rain Index pattern)\n    http_session = requests.Session()\n\n    def fetch_single(call_info, attempt=1):\n        \"\"\"Fetch premium rates for one grid/year/coverage combination.\"\"\"\n        params = {\n            \"intervalType\": \"BiMonthly\",\n            \"irrigationPracticeCode\": irrigation_practice,\n            \"organicPracticeCode\": organic_practice,\n            \"intendedUseCode\": intended_use_code,\n            \"stateCode\": call_info['state_code'],\n            \"countyCode\": call_info['county_code'],\n            \"productivityFactor\": \"100\",\n            \"insurableInterest\": \"100\",\n            \"insuredAcres\": \"1000\",\n            \"sampleYear\": str(call_info['year']),\n            \"intervalPercentOfValues\": interval_percent,\n            \"coverageLevelPercent\": str(call_info['coverage_level']),\n            \"gridId\": str(call_info['grid_id']),\n            \"gridName\": str(call_info['grid_id'])\n        }\n\n        try:\n            resp = http_session.get(BASE_URL, params=params, timeout=request_timeout)\n            resp.raise_for_status()\n            data = resp.json()\n\n            if 'returnData' not in data or 'PricingRateRows' not in data['returnData']:\n                return [], f\"No PricingRateRows for Grid {call_info['grid_id']}, Year {call_info['year']}, Cov {call_info['coverage_level']}\", False\n\n            rows = []\n            for row in data['returnData']['PricingRateRows']:\n                code = row.get('IntervalCode')\n                if code and code != 'Total':\n                    rows.append((\n                        int(call_info['grid_id']),\n                        call_info['state_code'],\n                        call_info['county_code'],\n                        intended_use,\n                        f\"{call_info['coverage_level']}%\",\n                        call_info['year'],\n                        code,\n                        INTERVAL_MAP.get(code, code),\n                        row.get('PremiumRate'),\n                        run_timestamp\n                    ))\n            return rows, None, False\n\n        except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:\n            # Retry once on timeout/connection errors\n            if attempt == 1:\n                time.sleep(2)  # Brief backoff\n                return [], str(e), True  # Signal retry needed\n            return [], f\"Error Grid {call_info['grid_id']}, Year {call_info['year']}, Cov {call_info['coverage_level']} (attempt {attempt}): {str(e)}\", False\n\n        except Exception as e:\n            return [], f\"Error Grid {call_info['grid_id']}, Year {call_info['year']}, Cov {call_info['coverage_level']}: {str(e)}\", False\n\n    # Execute with thread pool\n    completed = 0\n    last_log_time = time.time()\n    LOG_INTERVAL_SECONDS = 30\n    retry_queue = []\n\n    with ThreadPoolExecutor(max_workers=max_concurrent) as executor:\n        futures = {executor.submit(fetch_single, call, 1): call for call in api_calls}\n\n        for future in as_completed(futures):\n            rows, error, needs_retry = future.result()\n            if rows:\n                all_rows.extend(rows)\n            if needs_retry:\n                retry_queue.append(futures[future])\n            elif error:\n                errors.append(error)\n            completed += 1\n\n            now = time.time()\n            if (now - last_log_time >= LOG_INTERVAL_SECONDS) or completed == total_calls:\n                pct = round(100 * completed / total_calls, 1)\n                elapsed = round(now - pipeline_start.timestamp())\n                log(\"FETCH\", f\"Progress: {completed}/{total_calls} ({pct}%) | {len(all_rows)} rows | {len(errors)} errors | {elapsed}s elapsed\")\n                last_log_time = now\n\n    # Process retries\n    if retry_queue:\n        log(\"RETRY\", f\"Retrying {len(retry_queue)} failed calls...\")\n        with ThreadPoolExecutor(max_workers=max_concurrent) as executor:\n            futures = {executor.submit(fetch_single, call, 2): call for call in retry_queue}\n            for future in as_completed(futures):\n                rows, error, _ = future.result()\n                if rows:\n                    all_rows.extend(rows)\n                    retried += 1\n                if error:\n                    errors.append(error)\n        log(\"RETRY\", f\"Recovered {retried} calls on retry.\")\n\n    # Close the shared HTTP session\n    http_session.close()\n\n    log(\"FETCH\", f\"Fetch complete: {len(all_rows)} total rows, {len(errors)} errors, {retried} recovered on retry\")\n\n    if errors and len(errors) <= 10:\n        for e in errors:\n            print(f\"    ! {e}\")\n    elif errors:\n        log(\"FETCH\", f\"Showing first 10 of {len(errors)} errors\")\n        for e in errors[:10]:\n            print(f\"    ! {e}\")\n\n    if not all_rows:\n        log(\"ERROR\", f\"No data returned. {len(errors)} errors.\")\n        return f\"COMPLETE: No data returned. {len(errors)} errors.\"\n\n    # ================================================================\n    # STEP 5: WRITE TO SNOWFLAKE\n    # ================================================================\n    log(\"WRITE\", f\"Writing {len(all_rows)} rows (mode={mode})...\")\n\n    columns = [\n        \"GRID_ID\", \"STATE_CODE\", \"COUNTY_CODE\", \"INTENDED_USE\",\n        \"COVERAGE_LEVEL\", \"YEAR\", \"INDEX_INTERVAL_CODE\",\n        \"INDEX_INTERVAL_NAME\", \"PREMIUMRATE\", \"INSERT_TIMESTAMP\"\n    ]\n\n    df = session.create_dataframe(all_rows, schema=columns)\n\n    if mode == 'OVERWRITE':\n        df.write.mode(\"overwrite\").save_as_table(TARGET_TABLE)\n        log(\"WRITE\", f\"OVERWRITE complete: {len(all_rows)} rows.\")\n\n    elif mode == 'APPEND':\n        df.write.mode(\"append\").save_as_table(TARGET_TABLE)\n        log(\"WRITE\", f\"APPEND complete: {len(all_rows)} rows added.\")\n\n    elif mode == 'MERGE':\n        # Use batch-specific staging table to avoid conflicts with parallel runs\n        if grid_batch:\n            batch_id = grid_batch.lower().replace(' ', '_').replace('of', '')\n            STAGING_TABLE_USED = f\"{STAGING_TABLE}_{batch_id}\"\n        else:\n            STAGING_TABLE_USED = STAGING_TABLE\n\n        df.write.mode(\"overwrite\").save_as_table(STAGING_TABLE_USED)\n\n        merge_sql = f\"\"\"\n            MERGE INTO {TARGET_TABLE} AS tgt\n            USING {STAGING_TABLE_USED} AS src\n            ON  tgt.GRID_ID              = src.GRID_ID\n            AND tgt.YEAR                 = src.YEAR\n            AND tgt.COVERAGE_LEVEL       = src.COVERAGE_LEVEL\n            AND tgt.INDEX_INTERVAL_CODE  = src.INDEX_INTERVAL_CODE\n            AND tgt.INTENDED_USE         = src.INTENDED_USE\n            WHEN MATCHED THEN UPDATE SET\n                tgt.STATE_CODE          = src.STATE_CODE,\n                tgt.COUNTY_CODE         = src.COUNTY_CODE,\n                tgt.INDEX_INTERVAL_NAME = src.INDEX_INTERVAL_NAME,\n                tgt.PREMIUMRATE         = src.PREMIUMRATE,\n                tgt.INSERT_TIMESTAMP    = src.INSERT_TIMESTAMP\n            WHEN NOT MATCHED THEN INSERT (\n                GRID_ID, STATE_CODE, COUNTY_CODE, INTENDED_USE,\n                COVERAGE_LEVEL, YEAR, INDEX_INTERVAL_CODE,\n                INDEX_INTERVAL_NAME, PREMIUMRATE, INSERT_TIMESTAMP\n            ) VALUES (\n                src.GRID_ID, src.STATE_CODE, src.COUNTY_CODE, src.INTENDED_USE,\n                src.COVERAGE_LEVEL, src.YEAR, src.INDEX_INTERVAL_CODE,\n                src.INDEX_INTERVAL_NAME, src.PREMIUMRATE, src.INSERT_TIMESTAMP\n            )\n        \"\"\"\n        result = session.sql(merge_sql).collect()\n        session.sql(f\"DROP TABLE IF EXISTS {STAGING_TABLE_USED}\").collect()\n        log(\"WRITE\", f\"MERGE complete: {result}\")\n\n    # ================================================================\n    # SUMMARY\n    # ================================================================\n    elapsed = (datetime.datetime.now() - pipeline_start).total_seconds()\n    summary = (\n        f\"COMPLETE | Mode: {mode} | \"\n        f\"{len(resolved_grids)} grids | {len(all_rows)} rows | \"\n        f\"{len(errors)} errors | {retried} retries recovered | {elapsed:.1f}s\"\n        f\"{batch_label}\"\n    )\n    log(\"DONE\", summary)\n    return summary\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "91169572-37e0-4e47-ad3d-4fd0a575d5b4",
   "metadata": {
    "language": "sql",
    "name": "PREMIUMS_STEP_3"
   },
   "outputs": [],
   "source": "-- ====================================================================\n-- RUN_PRF_PREMIUM_PIPELINE — Parameter Guide\n-- ====================================================================\n--\n-- PARAMETER 1: STATE_NAMES (ARRAY or NULL)\n--   States to load premium rates for (resolves grids automatically)\n--   Examples: ARRAY_CONSTRUCT('Texas')\n--             ARRAY_CONSTRUCT('Texas', 'Oklahoma')\n--   Set to NULL if using GRID_IDS instead\n--\n-- PARAMETER 2: GRID_IDS (ARRAY or NULL)\n--   Specific grid IDs to load (use instead of STATE_NAMES)\n--   Examples: ARRAY_CONSTRUCT(9128, 9129, 9130)\n--   Set to NULL if using STATE_NAMES instead\n--\n-- PARAMETER 3: YEARS (ARRAY)\n--   Which policy years to fetch rates for\n--   Default: ARRAY_CONSTRUCT(2025)\n--\n-- PARAMETER 4: COVERAGE_LEVELS (ARRAY)\n--   Coverage level percentages to fetch\n--   Default: ARRAY_CONSTRUCT(70, 75, 80, 85, 90)\n--\n-- PARAMETER 5: INTENDED_USE (VARCHAR)\n--   Friendly label — 'Grazing' or 'Haying'\n--   Default: 'Grazing'\n--\n-- PARAMETER 6: INTENDED_USE_CODE (VARCHAR)\n--   RMA code — '007' for Grazing, '006' for Haying\n--   Default: '007'\n--\n-- PARAMETER 7: IRRIGATION_PRACTICE (VARCHAR)\n--   '997' = Non-irrigated (standard for PRF)\n--   Default: '997'\n--\n-- PARAMETER 8: ORGANIC_PRACTICE (VARCHAR)\n--   '997' = Not specified (standard for PRF)\n--   Default: '997'\n--\n-- PARAMETER 9: INTERVAL_PERCENT (VARCHAR)\n--   JSON array of 11 values (percent allocation per interval)\n--   Default: '[50,0,50,0,0,0,0,0,0,0,0]'\n--   NOTE: This affects rate calculation. Use a balanced default.\n--\n-- PARAMETER 10: MAX_CONCURRENT (INT)\n--   Parallel API threads (5-10 recommended)\n--   Default: 5\n--\n-- PARAMETER 11: REQUEST_TIMEOUT (INT)\n--   Seconds per API call before timeout\n--   Default: 60\n--\n-- PARAMETER 12: MODE (VARCHAR)\n--   'OVERWRITE' — Drop and reload all data\n--   'APPEND'    — Add rows (risk of duplicates)\n--   'MERGE'     — Upsert on GRID_ID + YEAR + COVERAGE_LEVEL +\n--                 INDEX_INTERVAL_CODE + INTENDED_USE\n-- ====================================================================\n\nCALL RUN_PRF_PREMIUM_PIPELINE(\n    ARRAY_CONSTRUCT('Oklahoma'),                  -- 1.  STATE_NAMES\n    NULL,                                      -- 2.  GRID_IDS (NULL = use states)\n    ARRAY_CONSTRUCT(2025),                     -- 3.  YEARS\n    ARRAY_CONSTRUCT(70, 75, 80, 85, 90),       -- 4.  COVERAGE_LEVELS\n    'Grazing',                                 -- 5.  INTENDED_USE\n    '007',                                     -- 6.  INTENDED_USE_CODE\n    '997',                                     -- 7.  IRRIGATION_PRACTICE\n    '997',                                     -- 8.  ORGANIC_PRACTICE\n    '[50,0,50,0,0,0,0,0,0,0,0]',              -- 9.  INTERVAL_PERCENT\n    10,                                         -- 10. MAX_CONCURRENT\n    60,                                        -- 11. REQUEST_TIMEOUT\n    'OVERWRITE'                                -- 12. MODE\n);\n\n-- ====================================================================\n-- OTHER EXAMPLES (uncomment to use)\n-- ====================================================================\n\n-- === King Ranch grids only (MERGE mode) ===\n-- CALL RUN_PRF_PREMIUM_PIPELINE(\n--     NULL,\n--     ARRAY_CONSTRUCT(9128, 9129, 9130, 9131, 8828, 8829, 8830, 8831,\n--                     8528, 8529, 8530, 8531, 8228, 8229, 8230, 8231,\n--                     7928, 7929, 7930, 7931),\n--     ARRAY_CONSTRUCT(2025),\n--     ARRAY_CONSTRUCT(70, 75, 80, 85, 90),\n--     'Grazing', '007', '997', '997',\n--     '[50,0,50,0,0,0,0,0,0,0,0]',\n--     5, 60, 'MERGE'\n-- );\n\n-- === Multi-state load ===\n-- CALL RUN_PRF_PREMIUM_PIPELINE(\n--     ARRAY_CONSTRUCT('Texas', 'Oklahoma', 'New Mexico'),\n--     NULL,\n--     ARRAY_CONSTRUCT(2025),\n--     ARRAY_CONSTRUCT(70, 75, 80, 85, 90),\n--     'Grazing', '007', '997', '997',\n--     '[50,0,50,0,0,0,0,0,0,0,0]',\n--     10, 60, 'MERGE'\n-- );",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "218a1272-b757-48cc-baaf-b3e52e24439f",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": "-- ====================================================================\n-- OTHER EXAMPLES (uncomment to use)\n-- ====================================================================\n\n-- === King Ranch grids only (MERGE mode) ===\n CALL RUN_PRF_PREMIUM_PIPELINE(\n     NULL,\n     ARRAY_CONSTRUCT(9128, 9129, 9130, 9131, 8828, 8829, 8830, 8831,\n                     8528, 8529, 8530, 8531, 8228, 8229, 8230, 8231,\n                     7928, 7929, 7930, 7931),\n     ARRAY_CONSTRUCT(2025),\n     ARRAY_CONSTRUCT(70, 75, 80, 85, 90),\n     'Grazing', '007', '997', '997',\n     '[50,0,50,0,0,0,0,0,0,0,0]',\n     10, 60, 'MERGE'\n );",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7ae5e579-45b6-446d-b67b-814cdc474325",
   "metadata": {
    "language": "sql",
    "name": "PREMIUMS_STEP_4"
   },
   "outputs": [],
   "source": "-- ====================================================================\n-- AUDIT: Verify loaded data\n-- ====================================================================\n\n-- 1. Summary by state\nSELECT\n    STATE_CODE,\n    COUNT(DISTINCT GRID_ID)         AS GRIDS,\n    COUNT(DISTINCT YEAR)            AS YEARS,\n    COUNT(DISTINCT COVERAGE_LEVEL)  AS COVERAGE_LEVELS,\n    COUNT(*)                        AS TOTAL_ROWS\nFROM CAPITAL_MARKETS_SANDBOX.PUBLIC.PRF_PREMIUM_RATES\nGROUP BY STATE_CODE\nORDER BY STATE_CODE;\n\n-- 2. Completeness check: every grid should have 55 rows\n--    (11 intervals x 5 coverage levels per year)\nSELECT\n    GRID_ID,\n    YEAR,\n    COUNT(*) AS ROW_COUNT,\n    CASE WHEN COUNT(*) = 55 THEN 'COMPLETE' ELSE 'INCOMPLETE' END AS STATUS\nFROM CAPITAL_MARKETS_SANDBOX.PUBLIC.PRF_PREMIUM_RATES\nGROUP BY GRID_ID, YEAR\nHAVING COUNT(*) != 55\nORDER BY GRID_ID, YEAR;\n\n-- 3. Sample records\nSELECT *\nFROM CAPITAL_MARKETS_SANDBOX.PUBLIC.PRF_PREMIUM_RATES\nORDER BY GRID_ID, YEAR, COVERAGE_LEVEL, INDEX_INTERVAL_CODE\nLIMIT 100;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46076d7e-e815-485c-914b-37ef5fbd3d1e",
   "metadata": {
    "language": "sql",
    "name": "PREMIUMS_AUDIT"
   },
   "outputs": [],
   "source": "DROP PROCEDURE IF EXISTS CAPITAL_MARKETS_SANDBOX.PUBLIC.RUN_PRF_PREMIUM_PIPELINE(\n    ARRAY, ARRAY, ARRAY, ARRAY, VARCHAR, VARCHAR, VARCHAR, VARCHAR, VARCHAR, INT, INT, VARCHAR\n);",
   "execution_count": null
  }
 ]
}